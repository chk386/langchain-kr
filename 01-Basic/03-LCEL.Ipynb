{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 예시: 프롬프트 + 모델 + 출력 파서\n",
    "\n",
    "가장 기본적이고 일반적인 사용 사례는 prompt 템플릿과 모델을 함께 연결하는 것입니다. 이것이 어떻게 작동하는지 보기 위해, 각 나라별 수도를 물어보는 Chain을 생성해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Users/haekyucho/projects/python/langchain-kr/.venv/lib/python3.11/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"NHN COMMERCE\", set_enable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 템플릿의 활용\n",
    "\n",
    "`PromptTemplate`\n",
    "\n",
    "- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n",
    "- 사용법\n",
    "  - `template`: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 `{}`는 변수를 나타냅니다.\n",
    "  - `input_variables`: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response  # 스트리밍 출력\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_template()` 메소드를 사용하여 PromptTemplate 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"미국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain 생성\n",
    "\n",
    "### LCEL(LangChain Expression Language)\n",
    "\n",
    "![lcel.png](./images/lcel.png)\n",
    "\n",
    "여기서 우리는 LCEL을 사용하여 다양한 구성 요소를 단일 체인으로 결합합니다\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "`|` 기호는 [unix 파이프 연산자](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>)와 유사하며, 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달합니다.\n",
    "\n",
    "이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달됩니다. 각 구성 요소를 개별적으로 살펴보면 무슨 일이 일어나고 있는지 이해할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.1)\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke() 호출\n",
    "\n",
    "- python 딕셔너리 형태로 입력값을 전달합니다.(키: 값)\n",
    "- invoke() 함수 호출 시, 입력값을 전달합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'으로 설정합니다.\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인공지능 모델의 학습 원리는 마치 어린아이가 배우는 것과 비슷합니다.  아이가 \"강아지\"라는 단어와 강아지 사진을 여러 번 반복해서 보면서 \"강아지\"라는 단어가 무엇을 의미하는지 배우는 것처럼, AI 모델도 데이터를 통해 배우죠.\\n\\n핵심은 **데이터**와 **피드백**입니다.\\n\\n1. **데이터 입력:**  AI 모델은 방대한 양의 데이터(예: 강아지 사진, 고양이 사진, 설명 등)를 입력받습니다. 이 데이터는 모델이 학습할 \"교재\"와 같습니다.\\n\\n2. **모델의 예측:** 모델은 입력된 데이터를 바탕으로 스스로 \"추측\"을 합니다.  처음에는 아무것도 모르기 때문에 무작위로 추측하거나, 매우 부정확한 예측을 할 수 있습니다.  (예: 강아지 사진을 보고 \"고양이\"라고 잘못 예측할 수 있습니다.)\\n\\n3. **피드백 (오차 수정):**  모델의 예측이 정답과 얼마나 다른지, 즉 \"오차\"를 계산합니다.  정답을 알려주는 \"정답지\"가 필요한 셈입니다.  이 오차를 바탕으로 모델의 내부 매개변수(가중치와 편향 등)를 조정합니다.  오차가 클수록 매개변수를 크게 조정합니다. 이 과정을 **학습**이라고 합니다.\\n\\n4. **반복:** 2번과 3번 과정을 수없이 반복합니다.  반복 학습을 통해 모델은 오차를 줄이고, 점점 더 정확한 예측을 할 수 있게 됩니다.  마치 아이가 강아지 사진을 많이 볼수록 강아지를 더 잘 구분하게 되는 것과 같습니다.\\n\\n**간단한 비유:**\\n\\n활쏘기를 배우는 것을 생각해보세요.\\n\\n* **데이터:** 과녁과 화살\\n* **모델:** 궁수\\n* **예측:** 화살이 과녁에 맞는 위치\\n* **피드백:** 과녁에 맞은 위치와 중심의 차이 (오차)\\n* **학습:** 오차를 줄이기 위해 자세나 힘 조절을 수정하는 것\\n\\n이 과정을 통해 궁수(모델)는 점점 더 과녁 중심에 화살을 맞출 수 있게 됩니다.  AI 모델도 마찬가지로, 데이터와 피드백을 통해 점점 더 정확한 예측을 할 수 있도록 학습하는 것입니다.  다만, AI 모델은 수많은 매개변수를 복잡한 수학적 알고리즘을 통해 조정합니다.\\n\\n\\n이 설명이 인공지능 모델의 학습 원리를 이해하는 데 도움이 되었기를 바랍니다.  물론, 실제 학습 과정은 훨씬 복잡하지만, 기본적인 원리는 이와 같습니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-2efec3fb-e5fc-4319-9427-9740a50934a9-0', usage_metadata={'input_tokens': 20, 'output_tokens': 697, 'total_tokens': 717, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 스트리밍을 출력하는 예시 입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력파서(Output Parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain 에 출력파서를 추가합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능 모델의 학습 원리는 마치 어린아이가 배우는 것과 비슷합니다.  아이가 \"강아지\"라는 단어와 강아지 사진을 여러 번 보면서 \"강아지\"라는 단어가 무엇을 의미하는지 배우는 것처럼, AI 모델도 데이터를 통해 배우죠.\\n\\n핵심은 **데이터**와 **피드백**입니다.\\n\\n1. **데이터 입력:**  AI 모델은 방대한 양의 데이터를 입력받습니다. 예를 들어, 고양이와 강아지를 구분하는 모델을 학습시키려면 수많은 고양이와 강아지 사진이 필요합니다.  이 데이터에는 사진 자체뿐 아니라 \"고양이\", \"강아지\"와 같은 라벨(정답)도 포함됩니다.\\n\\n2. **모델의 추론:** 모델은 입력된 데이터를 바탕으로 스스로 \"고양이\"와 \"강아지\"를 구분하는 방법을 찾아냅니다. 처음에는 무작위로 추측할 수도 있지만, 학습 과정을 거치면서 점점 정확해집니다.  이 과정을 **추론(Inference)**이라고 합니다.\\n\\n3. **오차 계산:** 모델의 추론 결과와 실제 정답(라벨)을 비교하여 오차(얼마나 틀렸는지)를 계산합니다.  오차가 크다는 것은 모델이 잘못 예측했다는 뜻입니다.\\n\\n4. **가중치 조정 (학습):**  오차를 줄이기 위해 모델 내부의 **가중치(weight)**와 **편향(bias)**를 조정합니다.  가중치와 편향은 모델이 데이터의 특징을 얼마나 중요하게 고려할지 결정하는 값입니다.  이 조정 과정은 **역전파(Backpropagation)**라는 알고리즘을 통해 이루어집니다.  쉽게 말해, 틀린 부분을 수정하는 과정입니다.\\n\\n5. **반복:** 1~4번 과정을 수없이 반복합니다.  데이터를 여러 번 반복해서 학습시키면서 오차를 점점 줄여나가고, 모델의 정확도를 높입니다.  이 과정을 **훈련(Training)**이라고 합니다.\\n\\n결국, AI 모델은 데이터를 통해 패턴을 학습하고, 그 패턴을 이용하여 새로운 데이터에 대해 예측하는 것입니다.  마치 아이가 강아지의 특징(털, 귀, 꼬리 등)을 배우고, 새로운 강아지를 보더라도 강아지라고 인식하는 것과 같습니다.  다만 AI 모델은 수많은 데이터와 복잡한 알고리즘을 통해 훨씬 더 정교하게 학습합니다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}\n",
    "answer = chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿을 변경하여 적용\n",
    "\n",
    "- 아래의 프롬프트 내용을 얼마든지 **변경** 하여 테스트 해볼 수 있습니다.\n",
    "- `model_name` 역시 변경하여 테스트가 가능합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿을 이용하여 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 챗모델을 초기화합니다.\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 문자열 출력 파서를 초기화합니다.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **영어 회화:**\n",
      "\n",
      "**Scenario 1: Simple order**\n",
      "\n",
      "Waiter: Hi, welcome to [Restaurant Name]. Can I help you?\n",
      "Me: Yes, please. I'd like the [Dish Name], please.\n",
      "Waiter:  Okay, and would you like anything to drink?\n",
      "Me:  Yes, a [Drink Name], please.\n",
      "Waiter:  Great. I'll be right back with your order.\n",
      "\n",
      "\n",
      "**Scenario 2:  More detailed order with questions**\n",
      "\n",
      "Waiter: Hi there, welcome!  What can I get for you today?\n",
      "Me: Hi.  I'm looking at the [Dish Name].  Does it come with [Side Dish]?\n",
      "Waiter: Yes, it comes with [Side Dish].  Would you like any substitutions?\n",
      "Me:  Hmm, could I have [Substitution] instead of [Original Side Dish]?\n",
      "Waiter: Absolutely. And what about a drink?\n",
      "Me: I'll have a [Drink Name], please.\n",
      "Waiter:  Excellent. I'll put your order in.\n",
      "\n",
      "\n",
      "**Scenario 3:  Asking for recommendations**\n",
      "\n",
      "Waiter: Hello, welcome to [Restaurant Name].  What are you in the mood for today?\n",
      "Me:  Hi. I'm not sure, actually. What would you recommend?\n",
      "Waiter: We have a delicious [Dish Name], it's very popular. Or, if you prefer something lighter, we have a great [Dish Name].\n",
      "Me:  Hmm, the [Dish Name] sounds good.  What's in it?\n",
      "Waiter: It's [Description of Dish].  Would you like to try that?\n",
      "Me: Yes, please. And I'll have a [Drink Name] with that.\n",
      "\n",
      "\n",
      "- **한글 해석:**\n",
      "\n",
      "**시나리오 1: 간단한 주문**\n",
      "\n",
      "웨이터: 안녕하세요, [식당 이름]에 오신 것을 환영합니다. 무엇을 도와드릴까요?\n",
      "나: 네, 주문하고 싶습니다. [메뉴 이름] 주세요.\n",
      "웨이터: 알겠습니다. 음료는 뭐 드릴까요?\n",
      "나: 네, [음료 이름] 주세요.\n",
      "웨이터: 좋습니다. 주문 곧 가져다 드리겠습니다.\n",
      "\n",
      "\n",
      "**시나리오 2: 질문을 포함한 자세한 주문**\n",
      "\n",
      "웨이터: 안녕하세요, 어서오세요! 오늘 뭘 드릴까요?\n",
      "나: 안녕하세요. [메뉴 이름]을 보고 있는데요. [곁들임 음식]이 포함되어 있나요?\n",
      "웨이터: 네, [곁들임 음식]이 포함되어 있습니다. 다른 걸로 바꾸실 건 없으세요?\n",
      "나: 음, [원래 곁들임 음식] 대신 [대체 음식]으로 바꿀 수 있을까요?\n",
      "웨이터: 물론입니다. 음료는 어떻게 하시겠어요?\n",
      "나: [음료 이름]으로 하겠습니다.\n",
      "웨이터: 좋습니다. 주문 넣겠습니다.\n",
      "\n",
      "\n",
      "**시나리오 3: 추천 요청**\n",
      "\n",
      "웨이터: 안녕하세요, [식당 이름]에 오신 것을 환영합니다. 오늘 무엇을 드시고 싶으세요?\n",
      "나: 안녕하세요. 잘 모르겠어요. 뭐 추천해주시겠어요?\n",
      "웨이터: [메뉴 이름]이 아주 맛있습니다. 인기 메뉴입니다. 아니면, 가볍게 드시고 싶으시다면 [메뉴 이름]도 좋습니다.\n",
      "나: 음, [메뉴 이름]이 좋겠네요.  어떤 재료가 들어가나요?\n",
      "웨이터: [메뉴 설명]입니다. 드셔보시겠어요?\n",
      "나: 네, 주세요. 그리고 [음료 이름]도 함께 주세요.\n"
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "result = chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
